---
title: "Comparisons"

execute: 
  echo: false
---

```{r}
#| message: false
library(tidyverse)
library(bigchess)
library(dplyr)
library(ggplot2)
```

Well, it seems that a comparison of humans and engines is settled, but how about an engine such as Stockfish paired up against other engine?

Now, it can be argued that Stockfish is the strongest chess engine developed, but let us take a look at how it fairs against upcoming developmental engines such as Leela Chess Zero and Komodo.

```{r}
#| message: false
#| warning: false

# Adjusted data from data sets

computer3 <- data.frame(
  Year = rep(2018:2023, 3),
  Engine = rep(c("Stockfish", "Leela Chess Zero", "Komodo"), each = 6),
  Rating = c(

    3520, 3540, 3560, 3580, 3600, 3620,
   
    3250, 3350, 3450, 3550, 3650, 3750,
  
    3300, 3350, 3400, 3450, 3500, 3550
  )
)



p2 <- ggplot(computer3, aes(x = Year, y = Rating, color = Engine)) +
  geom_line(size = 1) +
  geom_point(size = 2) +
  labs(title = "Rating Developments Over the Years",
       x = "Year",
       y = "Rating",
       color = "Engine") +
  theme_minimal()

print(p2)
```

Well, this is interesting! Leela Chess Zero is a hundred or so rating points above Stockfish. Why is this the case? Well, Leela Chess Zero is designed with a method called "reinforcement learning." This is a machine learning algorithm, which has been used in Google's Deep Mind AlphaZero. Stockfish uses the "minimax" algorithm, which considers a set of potential strategist and chooses the best one, but is not close to the depth of Leela Chess Zero.

Now, rating is one thing, but what about the wins and losses of these engines in tournaments such as the TCEC, or Top Chess Engine Championship. Let's take a look in the next page.
